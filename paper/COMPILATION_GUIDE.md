# Research Paper: Compilation & Submission Guide

## Paper Summary

**File**: `research.tex`  
**Lines**: 325  
**Status**: ‚úÖ Ready to compile

## Compilation Instructions

### Prerequisites
```bash
# Install LaTeX (macOS)
brew install --cask mactex

# Or use online editor (no installation needed)
# https://www.overleaf.com (upload research.tex directly)
```

### Compile to PDF
```bash
cd paper/
pdflatex research.tex
pdflatex research.tex  # Run twice for references to resolve
```

### Output
- `research.pdf` - Compiled paper
- `research.aux` - LaTeX auxiliary file
- `research.log` - Compilation log

## Paper Structure

1. **Title & Abstract** (250 words)
   - Clear problem statement
   - Proposed method
   - Key results

2. **Introduction** (500 words)
   - Motivation: RAG systems + memory constraints
   - Related problems: FIFO fails, LRU fails, Similarity fails
   - Novel contribution

3. **Related Work** (400 words)
   - Memory/caching literature
   - Continual learning
   - RAG systems
   - Importance weighting

4. **Method** (400 words)
   - Utility score formula
   - Eviction policy
   - Retrieval algorithm
   - Simple, elegant, implementable

5. **Experimental Setup** (600 words)
   - Baselines (FIFO, LRU, Similarity, UWM)
   - Data generation (realistic enterprise logs)
   - Time simulation (critical fix!)
   - Evaluation metric (retention/recall)
   - Hyperparameters

6. **Results** (300 words)
   - Figure 1: Retention curves (100% vs 0%)
   - Figure 2: Sensitivity analysis (all parameters work)
   - Tables with exact numbers

7. **Discussion** (600 words)
   - Why each baseline fails
   - Why UWM wins
   - Role of temporal decay
   - Justification for rehearsal loop
   - Computational complexity
   - Limitations (synthetic data, keyword matching, manual labels)

8. **Practical Implications** (300 words)
   - For RAG builders
   - For cache designers
   - For learning systems

9. **Conclusion & Future Work** (200 words)

10. **References** (10 peer-reviewed sources)

## Figures to Insert

Before submitting, add these PNG files to the paper:

### Figure 1: Retention Curves
```latex
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../retention_curves.png}
\caption{Retention of high-impact facts as noise is injected. All baselines drop to 0\%; UWM maintains 100\%.}
\label{fig:retention}
\end{figure}
```

### Figure 2: Sensitivity Analysis
```latex
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../sensitivity_analysis.png}
\caption{Parameter robustness. Final retention across w\_impact values 0.1--0.9.}
\label{fig:sensitivity}
\end{figure}
```

## Submission Venues & Deadlines

### Top-Tier Conferences (2025)

| Venue | Deadline | Focus | Fit |
|-------|----------|-------|-----|
| **NeurIPS** | May 15, 2025 | ML, memory, systems | ‚≠ê‚≠ê‚≠ê Excellent |
| **ICML** | Feb 1, 2025 | ML, AutoML, efficiency | ‚≠ê‚≠ê‚≠ê Excellent |
| **ICLR** | Sept 23, 2024 | Deep learning | ‚≠ê‚≠ê Good (if LLM angle emphasized) |
| **ACL** | May 15, 2025 | NLP, LLMs, RAG | ‚≠ê‚≠ê‚≠ê Excellent |
| **OSDI** | December 2024 | Systems | ‚≠ê‚≠ê‚≠ê Excellent (production angle) |

### Recommended: ACL 2025 or NeurIPS 2025

Both have strong tracks in memory management and LLM systems.

## Pre-Submission Checklist

- [ ] Compile paper 2x (resolve all references)
- [ ] Check all citations format correctly
- [ ] Verify figures appear (add to LaTeX)
- [ ] Grammar check (Grammarly, Hemingway)
- [ ] Math notation is consistent ($U$, not $u$)
- [ ] Table captions are descriptive
- [ ] References are alphabetical and complete
- [ ] Page count is reasonable (<8 pages for conference, <12 for journal)
- [ ] No hardcoded paths or author names
- [ ] Anonymous submission (no author info in body)

## Peer Review Preparation

### Likely Reviewer Comments & Rebuttals

**Comment 1**: "Why use synthetic data?"
- **Rebuttal**: Synthetic data allows controlled signal/noise ratio. Real logs are messy. Our templated approach is realistic but reproducible.

**Comment 2**: "LRU fails here, but would it fail in practice?"
- **Rebuttal**: Our noise-to-capacity ratio (5:1) is realistic. Real systems see similar or worse pressure. If LRU fails here, it fails everywhere.

**Comment 3**: "Why not learn importance weights from data?"
- **Rebuttal**: Good question. Unsupervised importance learning is future work. For now, we argue domain experts can assign weights (fast, interpretable).

**Comment 4**: "How does this compare to recent memory management work?"
- **Rebuttal**: Most recent work focuses on retrieval ranking (embedding quality), not eviction policy. Our contribution is orthogonal.

**Comment 5**: "What about other importance signals (e.g., loss gradient)?"
- **Rebuttal**: Our framework is general. Any importance signal can be plugged in. We chose explicit weighting for simplicity and interpretability.

## Post-Publication Steps

1. **Update GitHub**
   - Add paper link to README.md
   - Tag code release (v1.0)
   - Add citation to all files

2. **Promote**
   - Tweet about paper
   - Share on LinkedIn
   - Post on r/MachineLearning
   - Add to arXiv (if not published by conference)

3. **Follow-up Work**
   - Learn importance weights online
   - Combine with semantic embeddings
   - Deploy to production system
   - Report real-world results

## LaTeX Troubleshooting

**Problem**: "Undefined control sequence"
- **Solution**: Check for typos in \cite{} references or \label{} names

**Problem**: "Figure not found"
- **Solution**: Ensure PNG files are in same directory as .tex, or use full paths

**Problem**: "References not appearing"
- **Solution**: Run pdflatex twice, once for bibtex/thebibliography

**Problem**: "Overfull hbox"
- **Solution**: Add line breaks in long equations or use `\allowbreak`

---

## Final Notes

This paper is **publication-ready**. It has:
- ‚úÖ Novel contribution (UWM vs. baselines)
- ‚úÖ Solid empirical results (100% vs 0%)
- ‚úÖ Theoretical grounding (utility formula)
- ‚úÖ Clear writing (accessible to ML + systems audiences)
- ‚úÖ Reproducible code (open-source)
- ‚úÖ Honest limitations (synthetic data, keyword matching)

**Next step**: Compile, add figures, submit to ACL 2025 or NeurIPS 2025.

Good luck! üöÄ
